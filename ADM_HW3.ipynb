{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Places of the world\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection\n",
    "\n",
    "For this homework, there is no provided dataset. Instead, you have to build your own. Your search engine will run on text documents. So, here\n",
    "we detail the procedure to follow for the data collection. \n",
    "\n",
    "\n",
    "### 1.2. Crawl places\n",
    "\n",
    "Once you get all the URLs in the first 400 pages of the list, you:\n",
    "\n",
    "1. Download the HTML corresponding to each of the collected URLs.\n",
    "2. After you collect a single page, immediately save its `HTML` in a file. In this way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
    "3. Organize the entire set of downloaded `HTML` pages into folders. Each folder will contain the `HTML` of the places on page 1, page 2, ... of the list of locations.\n",
    "\n",
    "__Tip__: Due to a large number of pages you should download, you can use some methods that can help you shorten the time it takes. If you employed a particular process or approach, kindly describe it.\n",
    " \n",
    "### 1.3 Parse downloaded pages\n",
    "\n",
    "At this point, you should have all the HTML documents about the places of interest, and you can start to extract the places' information. The list of the information we desire for each place and their format is as follows:\n",
    "\n",
    "1. Place Name (to save as `placeName`): String.\n",
    "2. Place Tags (to save as `placeTags`): List of Strings.\n",
    "3. \\# of people who have been there (to save as `numPeopleVisited`): Integer.\n",
    "4. \\# of people who want to visit the place(to save as `numPeopleWant`): Integer.\n",
    "5. Description (to save as `placeDesc`): String. Everything from under the first image up to \"know before you go\" (orange frame on the example image).\n",
    "6. Short Description (to save as `placeShortDesc`): String. Everything from the title and location up to the image (blue frame on the example image).\n",
    "7. Nearby Places (to save as `placeNearby`): Extract the names of all nearby places, but only keep unique values: List of Strings.\n",
    "8. Address of the place(to save as `placeAddress`): String.\n",
    "9. Altitude and Longitude of the place's location(to save as `placeAlt` and `placeLong`): Integers\n",
    "10. The username of the post editors (to save as `placeEditors`): List of Strings.\n",
    "11. Post publishing date (to save as `placePubDate`): datetime.\n",
    "12. The names of the lists that the place was included in (to save as `placeRelatedLists`): List of Strings.\n",
    "13. The names of the related places (to save as `placeRelatedPlaces`): List of Strings.\n",
    "14. The URL of the page of the place (to save as `placeURL`):String\n",
    "<p align=\"center\">\n",
    "<img src=\"img/last_version_place.png\" width = 1000>\n",
    "</p>\n",
    "\n",
    "\n",
    "For each place, you create a `place_i.tsv` file of this structure:\n",
    "\n",
    "```\n",
    "placeName \\t placeTags \\t  ... \\t placeURL\n",
    "```\n",
    "\n",
    "If an information is missing, you just leave it as an empty string.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of places\n",
    "\n",
    "We start with the list of places to include in your corpus of documents. In particular, we focus on the [Most popular places](https://www.atlasobscura.com/places?sort=likes_count). Next, we want you to **collect the URL** associated with each site in the list from this list.\n",
    "The list is long and split into many pages. Therefore, we ask you to retrieve only the URLs of the places listed in **the first 400 pages** (each page has 18 places, so that you will end up with 7200 unique place URLs).\n",
    "\n",
    "The output of this step is a `.txt` file whose single line corresponds to the place's URL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built the urls i need to get every url for every places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url  = 'https://www.atlasobscura.com/places?sort=likes_count'\n",
    "main_domain  ='https://www.atlasobscura.com'\n",
    "get_page_query = '/places?pages?page='\n",
    "query_end = '&sort=likes_count'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code to get every url of evry places and the save them locally in places_url.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_url  = []\n",
    "output = open(\"places_url.txt\",'w')\n",
    "cnt  =  0\n",
    "for i in range(1,401):\n",
    "    req  = requests.get(main_domain+get_page_query+str(i)+query_end)\n",
    "    soup  = BeautifulSoup(req.text)\n",
    "    places_url= [main_domain+ x.get('href') for x in soup.find_all('a',{'class' : 'content-card-place' })]\n",
    "    for url in places_url:\n",
    "        output.write(url+\"\\n\")\n",
    "        cnt+=1\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all the 7200 urls have been written in places_url.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200\n"
     ]
    }
   ],
   "source": [
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl places\n",
    "\n",
    "Once you get all the URLs in the first 400 pages of the list, you:\n",
    "\n",
    "1. Download the HTML corresponding to each of the collected URLs.\n",
    "2. After you collect a single page, immediately save its `HTML` in a file. In this way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
    "3. Organize the entire set of downloaded `HTML` pages into folders. Each folder will contain the `HTML` of the places on page 1, page 2, ... of the list of locations.\n",
    "\n",
    "__Tip__: Due to a large number of pages you should download, you can use some methods that can help you shorten the time it takes. If you employed a particular process or approach, kindly describe it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_html function\n",
    "\n",
    "In this function there is the code to download the page html of a place on www.atlasobscura.com:\n",
    "\n",
    "*Parameters*\n",
    "\n",
    "    __url__ : this url is the location of the html page that we want to download, it is read from a file txt containing 7200 url of places on www.atlasobscura.com.\n",
    "    \n",
    "    __path__: this path is the location where we want to save the .html file locally.\n",
    "\n",
    "    __number__: this number goes from 0 to 7199, each number corresponds to a different place.\n",
    "    \n",
    "    __page__: this number goes from 1 to 400, each number corresponds to a page from this link https://www.atlasobscura.com/places?page=<page>sort=likes_count.\n",
    "\n",
    "*Execution*\n",
    "    \n",
    "    This function makes a GET request to the given <url>, tries to create a new html file called place<number>.html in the <path> given as input.\n",
    "    When the GET request has a status code equals to 200, the request.text attribute is written in the place<number>.html. \n",
    "    After the file is written, it gives a feedback if the file as already been saved locally( in this case you will see on the standard output \"Done!\") or if it has just been saved locally(in this case you will see on the standard output \"Downloaded place <number>, Page <page>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url,path,number,page):\n",
    "    try:    \n",
    "        if not os.path.exists(path+\"/place\"+str(number)+\".html\") or os.path.getsize(path+\"/place\"+str(number)+\".html\") < 2000:\n",
    "            with open(path+\"/place\"+str(number)+\".html\",\"w\", encoding=\"utf-8\") as file_html:\n",
    "                req =  requests.get(url)\n",
    "                if req.status_code == 200:\n",
    "                    file_html.write(req.text)\n",
    "                    print(\"Downloaded place \"+str(number)+\", Page \"+str(page))\n",
    "        else:\n",
    "            print(\"done!\")\n",
    "    except Exception as e:\n",
    "        print(\"Error occured! \"+ str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code creates the need 400 directory correponding to each page from this link  https://www.atlasobscura.com/places?page=<page_index>sort=likes_count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_index  in range(1,401):\n",
    "    dir_path  = \"HTML_FILES/page\"+str(page_index)\n",
    "    if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code the calls get_html for every url saved in the places_url.txt.\n",
    "I used linecache module to acces directly to a specific line in the file.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,7200):\n",
    "    page  = (i//18)+1\n",
    "    dir_path  = \"HTML_FILES/page\"+str(page)\n",
    "    url = str(linecache.getline('places_url.txt',i+1).replace('\\n',\"\"))\n",
    "    get_html(url,dir_path,i,page)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fe61594923d2a5d982534718f3a44d80d54aa84bc222edb79bd8e8f2cce8cae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
