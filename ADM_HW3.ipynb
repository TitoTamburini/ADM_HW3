{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Places of the world\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection\n",
    "\n",
    "For this homework, there is no provided dataset. Instead, you have to build your own. Your search engine will run on text documents. So, here\n",
    "we detail the procedure to follow for the data collection. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of places\n",
    "\n",
    "We start with the list of places to include in your corpus of documents. In particular, we focus on the [Most popular places](https://www.atlasobscura.com/places?sort=likes_count). Next, we want you to **collect the URL** associated with each site in the list from this list.\n",
    "The list is long and split into many pages. Therefore, we ask you to retrieve only the URLs of the places listed in **the first 400 pages** (each page has 18 places, so that you will end up with 7200 unique place URLs).\n",
    "\n",
    "The output of this step is a `.txt` file whose single line corresponds to the place's URL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built the urls i need to get every url for every places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url  = 'https://www.atlasobscura.com/places?sort=likes_count'\n",
    "main_domain  ='https://www.atlasobscura.com'\n",
    "get_page_query = '/places?page='\n",
    "query_end = '&sort=likes_count'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code to get every url of evry places and the save them locally in places_url.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_url  = []\n",
    "output = open(\"places_url.txt\",'w')\n",
    "cnt  =  0\n",
    "for i in range(1,401):\n",
    "    try:\n",
    "        req  = requests.get(main_domain+get_page_query+str(i)+query_end)\n",
    "        soup  = BeautifulSoup(req.text)\n",
    "        places_url= [main_domain+ x.get('href') for x in soup.find_all('a',{'class' : 'content-card-place' })]\n",
    "        output.write('\\n'.join(places_url))\n",
    "        cnt+=18\n",
    "    except Exception as e:\n",
    "        print(\"Error Occured: \"+e)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all the 7200 urls have been written in places_url.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200\n"
     ]
    }
   ],
   "source": [
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl places\n",
    "\n",
    "Once you get all the URLs in the first 400 pages of the list, you:\n",
    "\n",
    "1. Download the HTML corresponding to each of the collected URLs.\n",
    "2. After you collect a single page, immediately save its `HTML` in a file. In this way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
    "3. Organize the entire set of downloaded `HTML` pages into folders. Each folder will contain the `HTML` of the places on page 1, page 2, ... of the list of locations.\n",
    "\n",
    "__Tip__: Due to a large number of pages you should download, you can use some methods that can help you shorten the time it takes. If you employed a particular process or approach, kindly describe it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_html function\n",
    "\n",
    "In this function there is the code to download the page html of a place on www.atlasobscura.com:\n",
    "\n",
    "*Parameters*\n",
    "\n",
    "    __url__ : this url is the location of the html page that we want to download, it is read from a file txt containing 7200 url of places on www.atlasobscura.com.\n",
    "    \n",
    "    __path__: this path is the location where we want to save the .html file locally.\n",
    "\n",
    "    __number__: this number goes from 0 to 7199, each number corresponds to a different place.\n",
    "    \n",
    "    __page__: this number goes from 1 to 400, each number corresponds to a page from this link https://www.atlasobscura.com/places?page=<page>sort=likes_count.\n",
    "\n",
    "*Execution*\n",
    "    \n",
    "    This function makes a GET request to the given <url>, tries to create a new html file called place<number>.html in the <path> given as input.\n",
    "    When the GET request has a status code equals to 200, the request.text attribute is written in the place<number>.html. \n",
    "    After the file is written, it gives a feedback if the file as already been saved locally( in this case you will see on the standard output \"Done!\") or if it has just been saved locally(in this case you will see on the standard output \"Downloaded place <number>, Page <page>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url,path,number,page):\n",
    "    try:    \n",
    "        with open(path+\"/place\"+str(number)+\".html\",\"w\", encoding=\"utf-8\") as file_html:\n",
    "            req =  requests.get(url)\n",
    "            if req.status_code == 200:\n",
    "                file_html.write(req.text)\n",
    "                print(\"Downloaded place \"+str(number)+\", Page \"+str(page))\n",
    "    except Exception as e:\n",
    "        print(\"Error occured! \"+ str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code creates the need 400 directory correponding to each page from this link  https://www.atlasobscura.com/places?page=<page_index>sort=likes_count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_index  in range(1,401):\n",
    "    dir_path  = \"HTML_FILES/page\"+str(page_index)\n",
    "    if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code the calls get_html for every url saved in the places_url.txt.\n",
    "I used linecache module to acces directly to a specific line in the file.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,7200):\n",
    "    page  = (i//18)+1\n",
    "    dir_path  = \"HTML_FILES/page\"+str(page)\n",
    "    url = str(linecache.getline('places_url.txt',i+1).replace('\\n',\"\"))\n",
    "    if not os.path.exists(dir_path+\"/place\"+str(i)+\".html\") or os.path.getsize(dir_path+\"/place\"+str(i)+\".html\") < 2000:\n",
    "        get_html(url,dir_path,i,page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n",
    "\n",
    "At this point, you should have all the HTML documents about the places of interest, and you can start to extract the places' information. The list of the information we desire for each place and their format is as follows:\n",
    "\n",
    "1. Place Name (to save as `placeName`): String.\n",
    "2. Place Tags (to save as `placeTags`): List of Strings.\n",
    "3. \\# of people who have been there (to save as `numPeopleVisited`): Integer.\n",
    "4. \\# of people who want to visit the place(to save as `numPeopleWant`): Integer.\n",
    "5. Description (to save as `placeDesc`): String. Everything from under the first image up to \"know before you go\" (orange frame on the example image).\n",
    "6. Short Description (to save as `placeShortDesc`): String. Everything from the title and location up to the image (blue frame on the example image).\n",
    "7. Nearby Places (to save as `placeNearby`): Extract the names of all nearby places, but only keep unique values: List of Strings.\n",
    "8. Address of the place(to save as `placeAddress`): String.\n",
    "9. Altitude and Longitude of the place's location(to save as `placeAlt` and `placeLong`): Integers\n",
    "10. The username of the post editors (to save as `placeEditors`): List of Strings.\n",
    "11. Post publishing date (to save as `placePubDate`): datetime.\n",
    "12. The names of the lists that the place was included in (to save as `placeRelatedLists`): List of Strings.\n",
    "13. The names of the related places (to save as `placeRelatedPlaces`): List of Strings.\n",
    "14. The URL of the page of the place (to save as `placeURL`):String\n",
    "<p align=\"center\">\n",
    "<img src=\"img/last_version_place.png\" width = 1000>\n",
    "</p>\n",
    "\n",
    "\n",
    "For each place, you create a `place_i.tsv` file of this structure:\n",
    "\n",
    "```\n",
    "placeName \\t placeTags \\t  ... \\t placeURL\n",
    "```\n",
    "\n",
    "If an information is missing, you just leave it as an empty string.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORT FOR WEB-SCRAPING\n",
    "\n",
    "All the functions we used for web_scraping are written in the web_scraping_functions.py file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_scraping_functions import *\n",
    "import linecache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATE TSV FILES FOR EACH PLACES IN 'https://www.atlasobscura.com/places'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a directory for each page in 'https://www.atlasobscura.com/places', example: for *page 1* the directory will be named *page1*.\n",
    "\n",
    "All the directories will be inside a main directory called **TSV_FILES**.\n",
    "\n",
    "The tsv file for the *i-th place* is named as so: *\"place_i.tsv\"*. Each tsv file is saved in the directory corresponding to the page in the site where you can find the *i-th place*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EXPLANATION OF MAKE_PLACE_TSV FUNCTION: \n",
    "    \n",
    "After creating all the directories, it start a **for** loop to create for each place a tsv file, so **i** it goes from 0 to 7200.\n",
    "\n",
    "\n",
    "In each iteration of the **for** loop, we create the parameters for the make_place_tsv functions:\n",
    "\n",
    "**i** is the number of the place\n",
    "        \n",
    " **page** is an integer representing the number of the page in which is published the current place, we need this number to build the html path.\n",
    " \n",
    " **html_path** is the path in the homework folder where all the html file are stored, ordered by page.\n",
    "\n",
    " **tsv_path** is the path in the homework folder where we want to create the tsv file for the **i**-th place.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*def make_place_tsv(place_number,tsv_path,html_path,url):*  definition of make_place_tsv, written in web_scraping_functions.py\n",
    "\n",
    "**OUTPUT:** the place_**i**.tsv is created in the **TSV_FILES**/page**page** path. Structure of the tsv file created: placeName \\t placeTags \\t  ... \\t placeURL\n",
    "\n",
    "***SOME STATS:***   It took about 18 minutes to create 7200 tsv files.\n",
    "\n",
    "The function is written below, all the other functions used inside are written in web_scraping_functions.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_place_tsv(place_number,tsv_path,html_path,url):\n",
    "    if not os.path.exists(tsv_path+\"/place_\"+str(place_number)+'.tsv'): \n",
    "        print(\"Place \"+str(place_number)+\"Page \"+str((place_number//18)+1))\n",
    "        try:\n",
    "            with open(html_path,\"r\",encoding='utf-8') as html:\n",
    "                soup = BeautifulSoup(html,'lxml')\n",
    "                row = \"\"\n",
    "                row += str(get_placeName(soup))+' \\t'\n",
    "                row += str(get_placeTags(soup))+' \\t'\n",
    "                row += str(get_placePeopleVisited(soup))+' \\t'\n",
    "                row += str(get_placePeopleWant(soup))+' \\t'\n",
    "                row += ' '+str(get_placeDesc(soup))+' \\t'\n",
    "                row += ' '+str(get_placeShortDesc(soup))+' \\t'\n",
    "                row+=' '+str(get_placeNearby(soup))+' \\t'\n",
    "                row+=' '+str(get_placeAddress(soup))+' \\t'\n",
    "                placeAlt,placeLong = get_placeCoordinates(soup)\n",
    "                row +=' '+str(placeAlt)+' \\t'\n",
    "                row +=' '+str(placeLong)+' \\t'\n",
    "                row +=' '+str(get_placeEditors(soup))+' \\t'\n",
    "                row +=' '+str(get_placePubDate(soup))+' \\t'\n",
    "                row +=' '+str(get_placeRelatedLists(soup))+' \\t'\n",
    "                row+=' '+str(get_placeRelatedPlaces(soup))+' \\t'\n",
    "                row += ' '+url+' \\t'\n",
    "                #print(row)\n",
    "                try:\n",
    "                        with open(tsv_path+\"/place_\"+str(place_number)+'.tsv','w',encoding='utf-8') as f:\n",
    "                            f.write(row)\n",
    "                            print(\"csv created!\")\n",
    "                except  Exception as e:\n",
    "                    print(\"File not Created: \"+e)\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred: \"+str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tsv_directories()\n",
    "for i in range(0,7200):\n",
    "    page  = (i//18)+1\n",
    "    html_path = \"HTML_FILES/page\"+str(page)+\"\\place\"+str(i)+\".html\"\n",
    "    tsv_path  = \"TSV_FILES/page\"+str(page)\n",
    "    url  = str(linecache.getline('places_url.txt',i+1)).replace('\\n',\"\")\n",
    "    make_place_tsv(i+1,tsv_path,html_path,url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MERGE ALL THE TSV FILES IN ONE\n",
    "\n",
    "In this csv file each row represents a place and has this structure: \n",
    "     \n",
    "     \n",
    "     placeName \\t placeTags \\t placePeopleVisited \\t placePeopleWant \\t placeDesc \\t placeShortDesc \\t placeNearby \\t placeAddress \\t placeAlt \\t placeLong \\t placeEditors \\t placePubDate \\t placeRelatedLists \\t placeRelatedPlaces \\t placeURL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tsv():\n",
    "    with open('merged.tsv','w',encoding='utf-8') as merged:\n",
    "        merged.write('placeName \\t placeTags \\t placePeopleVisited \\t placePeopleWant \\t placeDesc \\t placeShortDesc \\t placeNearby \\t placeAddress \\t placeAlt \\t placeLong \\t placeEditors \\t placePubDate \\t placeRelatedLists \\t placeRelatedPlaces \\t placeURL\\n')\n",
    "        for i in range(0,7200):\n",
    "            page  = (i//18)+1\n",
    "            tsv_file  = \"TSV_FILES/page\"+str(page)+'/place_'+str(i+1)+'.tsv'\n",
    "            with open(tsv_file,'r',encoding='utf-8') as tsv : \n",
    "                merged.write(tsv.readline()+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_tsv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fe61594923d2a5d982534718f3a44d80d54aa84bc222edb79bd8e8f2cce8cae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
