{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Places of the world\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection\n",
    "\n",
    "For this homework, there is no provided dataset. Instead, you have to build your own. Your search engine will run on text documents. So, here\n",
    "we detail the procedure to follow for the data collection. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of places\n",
    "\n",
    "We start with the list of places to include in your corpus of documents. In particular, we focus on the [Most popular places](https://www.atlasobscura.com/places?sort=likes_count). Next, we want you to **collect the URL** associated with each site in the list from this list.\n",
    "The list is long and split into many pages. Therefore, we ask you to retrieve only the URLs of the places listed in **the first 400 pages** (each page has 18 places, so that you will end up with 7200 unique place URLs).\n",
    "\n",
    "The output of this step is a `.txt` file whose single line corresponds to the place's URL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built the urls i need to get every url for every places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url  = 'https://www.atlasobscura.com/places?sort=likes_count'\n",
    "main_domain  ='https://www.atlasobscura.com'\n",
    "get_page_query = '/places?page='\n",
    "query_end = '&sort=likes_count'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code to get every url of evry places and the save them locally in places_url.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_url  = []\n",
    "output = open(\"places_url.txt\",'w')\n",
    "cnt  =  0\n",
    "for i in range(1,401):\n",
    "    try:\n",
    "        req  = requests.get(main_domain+get_page_query+str(i)+query_end)\n",
    "        soup  = BeautifulSoup(req.text)\n",
    "        places_url= [main_domain+ x.get('href') for x in soup.find_all('a',{'class' : 'content-card-place' })]\n",
    "        output.write('\\n'.join(places_url))\n",
    "        cnt+=18\n",
    "    except Exception as e:\n",
    "        print(\"Error Occured: \"+e)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all the 7200 urls have been written in places_url.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200\n"
     ]
    }
   ],
   "source": [
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl places\n",
    "\n",
    "Once you get all the URLs in the first 400 pages of the list, you:\n",
    "\n",
    "1. Download the HTML corresponding to each of the collected URLs.\n",
    "2. After you collect a single page, immediately save its `HTML` in a file. In this way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
    "3. Organize the entire set of downloaded `HTML` pages into folders. Each folder will contain the `HTML` of the places on page 1, page 2, ... of the list of locations.\n",
    "\n",
    "__Tip__: Due to a large number of pages you should download, you can use some methods that can help you shorten the time it takes. If you employed a particular process or approach, kindly describe it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_html function\n",
    "\n",
    "In this function there is the code to download the page html of a place on www.atlasobscura.com:\n",
    "\n",
    "*Parameters*\n",
    "\n",
    "    __url__ : this url is the location of the html page that we want to download, it is read from a file txt containing 7200 url of places on www.atlasobscura.com.\n",
    "    \n",
    "    __path__: this path is the location where we want to save the .html file locally.\n",
    "\n",
    "    __number__: this number goes from 0 to 7199, each number corresponds to a different place.\n",
    "    \n",
    "    __page__: this number goes from 1 to 400, each number corresponds to a page from this link https://www.atlasobscura.com/places?page=<page>sort=likes_count.\n",
    "\n",
    "*Execution*\n",
    "    \n",
    "    This function makes a GET request to the given <url>, tries to create a new html file called place<number>.html in the <path> given as input.\n",
    "    When the GET request has a status code equals to 200, the request.text attribute is written in the place<number>.html. \n",
    "    After the file is written, it gives a feedback if the file as already been saved locally( in this case you will see on the standard output \"Done!\") or if it has just been saved locally(in this case you will see on the standard output \"Downloaded place <number>, Page <page>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url,path,number,page):\n",
    "    try:    \n",
    "        with open(path+\"/place\"+str(number)+\".html\",\"w\", encoding=\"utf-8\") as file_html:\n",
    "            req =  requests.get(url)\n",
    "            if req.status_code == 200:\n",
    "                file_html.write(req.text)\n",
    "                print(\"Downloaded place \"+str(number)+\", Page \"+str(page))\n",
    "    except Exception as e:\n",
    "        print(\"Error occured! \"+ str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code creates the need 400 directory correponding to each page from this link  https://www.atlasobscura.com/places?page=<page_index>sort=likes_count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_index  in range(1,401):\n",
    "    dir_path  = \"HTML_FILES/page\"+str(page_index)\n",
    "    if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code the calls get_html for every url saved in the places_url.txt.\n",
    "I used linecache module to acces directly to a specific line in the file.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,7200):\n",
    "    page  = (i//18)+1\n",
    "    dir_path  = \"HTML_FILES/page\"+str(page)\n",
    "    url = str(linecache.getline('places_url.txt',i+1).replace('\\n',\"\"))\n",
    "    if not os.path.exists(dir_path+\"/place\"+str(i)+\".html\") or os.path.getsize(dir_path+\"/place\"+str(i)+\".html\") < 2000:\n",
    "        get_html(url,dir_path,i,page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n",
    "\n",
    "At this point, you should have all the HTML documents about the places of interest, and you can start to extract the places' information. The list of the information we desire for each place and their format is as follows:\n",
    "\n",
    "1. Place Name (to save as `placeName`): String.\n",
    "2. Place Tags (to save as `placeTags`): List of Strings.\n",
    "3. \\# of people who have been there (to save as `numPeopleVisited`): Integer.\n",
    "4. \\# of people who want to visit the place(to save as `numPeopleWant`): Integer.\n",
    "5. Description (to save as `placeDesc`): String. Everything from under the first image up to \"know before you go\" (orange frame on the example image).\n",
    "6. Short Description (to save as `placeShortDesc`): String. Everything from the title and location up to the image (blue frame on the example image).\n",
    "7. Nearby Places (to save as `placeNearby`): Extract the names of all nearby places, but only keep unique values: List of Strings.\n",
    "8. Address of the place(to save as `placeAddress`): String.\n",
    "9. Altitude and Longitude of the place's location(to save as `placeAlt` and `placeLong`): Integers\n",
    "10. The username of the post editors (to save as `placeEditors`): List of Strings.\n",
    "11. Post publishing date (to save as `placePubDate`): datetime.\n",
    "12. The names of the lists that the place was included in (to save as `placeRelatedLists`): List of Strings.\n",
    "13. The names of the related places (to save as `placeRelatedPlaces`): List of Strings.\n",
    "14. The URL of the page of the place (to save as `placeURL`):String\n",
    "<p align=\"center\">\n",
    "<img src=\"img/last_version_place.png\" width = 1000>\n",
    "</p>\n",
    "\n",
    "\n",
    "For each place, you create a `place_i.tsv` file of this structure:\n",
    "\n",
    "```\n",
    "placeName \\t placeTags \\t  ... \\t placeURL\n",
    "```\n",
    "\n",
    "If an information is missing, you just leave it as an empty string.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORT FOR WEB-SCRAPING\n",
    "\n",
    "All the functions we used for web_scraping are written in the web_scraping_functions.py file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_scraping_functions import *\n",
    "import linecache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATE TSV FILES FOR EACH PLACES IN 'https://www.atlasobscura.com/places'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a directory for each page in 'https://www.atlasobscura.com/places', example: for *page 1* the directory will be named *page1*.\n",
    "\n",
    "All the directories will be inside a main directory called **TSV_FILES**.\n",
    "\n",
    "The tsv file for the *i-th place* is named as so: *\"place_i.tsv\"*. Each tsv file is saved in the directory corresponding to the page in the site where you can find the *i-th place*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EXPLANATION OF MAKE_PLACE_TSV FUNCTION: \n",
    "    \n",
    "After creating all the directories, it start a **for** loop to create for each place a tsv file, so **i** it goes from 0 to 7200.\n",
    "\n",
    "\n",
    "In each iteration of the **for** loop, we create the parameters for the make_place_tsv functions:\n",
    "\n",
    "**i** is the number of the place\n",
    "        \n",
    " **page** is an integer representing the number of the page in which is published the current place, we need this number to build the html path.\n",
    " \n",
    " **html_path** is the path in the homework folder where all the html file are stored, ordered by page.\n",
    "\n",
    " **tsv_path** is the path in the homework folder where we want to create the tsv file for the **i**-th place.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*def make_place_tsv(place_number,tsv_path,html_path,url):*  definition of make_place_tsv, written in web_scraping_functions.py\n",
    "\n",
    "**OUTPUT:** the place_**i**.tsv is created in the **TSV_FILES**/page**page** path. Structure of the tsv file created: placeName \\t placeTags \\t  ... \\t placeURL\n",
    "\n",
    "***SOME STATS:***   It took about 18 minutes to create 7200 tsv files.\n",
    "\n",
    "The function is written below, all the other functions used inside are written in web_scraping_functions.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_place_tsv(place_number,tsv_path,html_path,url):\n",
    "    if not os.path.exists(tsv_path+\"/place_\"+str(place_number)+'.tsv'): \n",
    "        print(\"Place \"+str(place_number)+\"Page \"+str((place_number//18)+1))\n",
    "        try:\n",
    "            with open(html_path,\"r\",encoding='utf-8') as html:\n",
    "                soup = BeautifulSoup(html,'lxml')\n",
    "                row = \"\"\n",
    "                row += str(get_placeName(soup))+' \\t'\n",
    "                row += str(get_placeTags(soup))+' \\t'\n",
    "                row += str(get_placePeopleVisited(soup))+' \\t'\n",
    "                row += str(get_placePeopleWant(soup))+' \\t'\n",
    "                row += ' '+str(get_placeDesc(soup))+' \\t'\n",
    "                row += ' '+str(get_placeShortDesc(soup))+' \\t'\n",
    "                row+=' '+str(get_placeNearby(soup))+' \\t'\n",
    "                row+=' '+str(get_placeAddress(soup))+' \\t'\n",
    "                placeAlt,placeLong = get_placeCoordinates(soup)\n",
    "                row +=' '+str(placeAlt)+' \\t'\n",
    "                row +=' '+str(placeLong)+' \\t'\n",
    "                row +=' '+str(get_placeEditors(soup))+' \\t'\n",
    "                row +=' '+str(get_placePubDate(soup))+' \\t'\n",
    "                row +=' '+str(get_placeRelatedLists(soup))+' \\t'\n",
    "                row+=' '+str(get_placeRelatedPlaces(soup))+' \\t'\n",
    "                row += ' '+url+' \\t'\n",
    "                #print(row)\n",
    "                try:\n",
    "                        with open(tsv_path+\"/place_\"+str(place_number)+'.tsv','w',encoding='utf-8') as f:\n",
    "                            f.write(row)\n",
    "                            print(\"csv created!\")\n",
    "                except  Exception as e:\n",
    "                    print(\"File not Created: \"+e)\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred: \"+str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tsv_directories()\n",
    "for i in range(0,7200):\n",
    "    page  = (i//18)+1\n",
    "    html_path = \"HTML_FILES/page\"+str(page)+\"\\place\"+str(i)+\".html\"\n",
    "    tsv_path  = \"TSV_FILES/page\"+str(page)\n",
    "    url  = str(linecache.getline('places_url.txt',i+1)).replace('\\n',\"\")\n",
    "    make_place_tsv(i+1,tsv_path,html_path,url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MERGE ALL THE TSV FILES IN ONE\n",
    "\n",
    "In this csv file each row represents a place and has this structure: \n",
    "     \n",
    "     \n",
    "     placeName \\t placeTags \\t placePeopleVisited \\t placePeopleWant \\t placeDesc \\t placeShortDesc \\t placeNearby \\t placeAddress \\t placeAlt \\t placeLong \\t placeEditors \\t placePubDate \\t placeRelatedLists \\t placeRelatedPlaces \\t placeURL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tsv():\n",
    "    with open('merged.tsv','w',encoding='utf-8') as merged:\n",
    "        merged.write('placeName \\t placeTags \\t placePeopleVisited \\t placePeopleWant \\t placeDesc \\t placeShortDesc \\t placeNearby \\t placeAddress \\t placeAlt \\t placeLong \\t placeEditors \\t placePubDate \\t placeRelatedLists \\t placeRelatedPlaces \\t placeURL\\n')\n",
    "        for i in range(0,7200):\n",
    "            page  = (i//18)+1\n",
    "            tsv_file  = \"TSV_FILES/page\"+str(page)+'/place_'+str(i+1)+'.tsv'\n",
    "            with open(tsv_file,'r',encoding='utf-8') as tsv : \n",
    "                merged.write(tsv.readline()+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_tsv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Theoretical question\n",
    "An imaginary university is interested in accepting some of the applicants for positions to study the Master of Data Science there. Unfortunately, only a few spots are available, so the university requires students to take some exams. Students are then admitted based on how well they perform on these exams. For students to determine whether they have been successfully accepted to the university, the university wants to create a ranking list that includes every student's first name, last name, and total average on its course webpage. Students should be ranked in the list based on their average points in descending order. For example, if two students have the same average punctuation, they should be sorted in ascending order using their first and last names. University will give you the students' information in __'ApplicantsInfo.txt'__ ([click here to download](https://adm2022.s3.amazonaws.com/ApplicantsInfo.txt)), and you should provide them with the ranking list in another *.txt* file and name it as __'RankingList.txt'__ . Kindly help this university in preparing this ranking list.\n",
    "\n",
    "**Input:** \n",
    "__'ApplicantsInfo.txt'__ will have the following format: \n",
    "- In the first line, you will be given *n* as the number of applicants and *m* as the number of exams that students have taken (all of them have taken the same exams), where: \n",
    "$$0 \\lt n \\le 5 * 10^4$$\n",
    "$$1 \\le m \\le 10^3$$\n",
    "- In each following *n* lines, you will find the information related to one of the students. Their first name, last name and *m* integers as the grades they received in *m* courses. \n",
    " \n",
    "**Output:**\n",
    "The output file should consist of __n__ lines, with each line representing one of the students and including the student's __first name, last name, and total average point__(setting the precision to 2 decimal points). As you know, they must be sorted in the order specified in the problem description. \n",
    "\n",
    "\n",
    "1. Try solving the problem mentioned above using three different sorting algorithms (do not use any MapReduce algorithm). (__Note:__ Built-in Python functions (like .mean, .sort, etc.) are not allowed to be used. You must implement the algorithms from scratch).\n",
    "2. What is the time complexity of each algorithm you have used?\n",
    "3. Evaluate the time taken for each of your implementations to answer the query stored in the __ApplicantsInfo.txt__ file and visualize them.\n",
    "4. What is the most optimal algorithm, in your opinion, and why?\n",
    "5. Implement a sorting algorithm using MapReduce and compare it against the three algorithms previously implemented using the __ApplicantsInfo.txt__ file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sorting Algorithms**\n",
    "\n",
    "To sort all the rows in the ___ApplicantsInfo.txt__, we implemented 3 sorting algorithm:\n",
    "\n",
    "1. __SelectionSort__\n",
    "2. __BubbleSort__\n",
    "3. __MergeSort__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_sort(values,N):\n",
    "\tfor i in range(N):\n",
    "\t\tmax_i = i\n",
    "\t\tfor  j in range(i+1,N):\n",
    "\t\t\tif( values[max_i][2] < values[j][2]):\n",
    "\t\t\t\tmax_i = j\n",
    "\t\t\telif (values[max_i][2] == values[j][2]):\n",
    "\t\t\t\tif((values[max_i][0]+values[max_i][1]).lower() > (values[j][0] +values[j][1]).lower()):\n",
    "\t\t\t\t\tmax_i = j\n",
    "\t\tvalues[i],values[max_i] = values[max_i],values[i]\n",
    "\n",
    "def swap(i,j,values):\n",
    "\ttemp =  values[i]\n",
    "\tvalues[i] = values[j]\n",
    "\tvalues[j] = temp\n",
    "\n",
    "def bubble_sort(values,N):\n",
    "\tfor i in range(N):\n",
    "\t\tfor j in range(N-i-1):\n",
    "\t\t\tif(values[j+1][2]>values[j][2]):\n",
    "\t\t\t\tswap(j,j+1,values)\n",
    "\t\t\telif(values[j+1][2]==values[j][2]):\n",
    "\t\t\t\tif((values[j+1][0]+values[j+1][1])<(values[j][0]+values[j][1])):\n",
    "\t\t\t\t\tswap(j,j+1,values)\n",
    "\n",
    "def merge_sort(values):\n",
    "\tif(len(values) > 1):\n",
    "\t\tmid  = len(values)//2\n",
    "\t\tl =  values[:mid]\n",
    "\t\tr = values[mid:]\n",
    "\t\tmerge_sort(l)\n",
    "\t\tmerge_sort(r)\n",
    "\t\til= 0\n",
    "\t\tir= 0 \n",
    "\t\ti = 0\n",
    "\t\twhile(il < len(l) and ir <len(r)):\n",
    "\t\t\tif( l[il][2] > r[ir][2]):\n",
    "\t\t\t\tvalues[i] = l[il]\n",
    "\t\t\t\til +=1\n",
    "\t\t\telif(l[il][2] == r[ir][2]):\n",
    "\t\t\t\tif(l[il][0]+l[il][1]).lower() < (r[ir][0] +r[ir][1]).lower():\n",
    "\t\t\t\t\tvalues[i]  = l[il]\n",
    "\t\t\t\t\til +=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tvalues[i]  = r[ir]\n",
    "\t\t\t\t\tir +=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tvalues[i] = r[ir]\n",
    "\t\t\t\tir +=1\n",
    "\t\t\ti+=1\n",
    "\t\t\n",
    "\t\twhile( il < len(l)):\n",
    "\t\t\tvalues[i] = l[il]\n",
    "\t\t\til += 1\n",
    "\t\t\ti+=1\n",
    "\t\t\t\n",
    "\t\twhile( ir<len(r)):\n",
    "\t\t\tvalues[i] = r[ir]\n",
    "\t\t\tir += 1\n",
    "\t\t\ti+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complexity**\n",
    "\n",
    "__SelectionSort__ : Given an array of size N, the selection sort algorithm, in the worst case scenario that the array is ordered in the opposite ordering we want, makes for every element of the array in position i, N-i-1 comparisons (i goes from 0 to N-1). Then swaps its position with the one of entry with the biggest mean in the section of the array that has not been ordered yet. \n",
    "\n",
    "\n",
    "*Number of comparisons* = $\\sum_{i = 1}^{N}$ $\\sum_{j=i+1}^{N} 1$ = $\\sum_{i=1}^{N}(N-(i+1)+1)$ = $\\sum_{i=1}^{N}(N-i)$ = $N(N-1)$ - $\\frac{N(N-1)}{2}$ = $\\frac{N(N-1)}{2}$ = $O$($N$^${2}$)\n",
    "\n",
    "__BubbleSort__ : Given an array of size N, the bubble sort algorithm, in the worst case scenario that the array is ordered in the opposite ordering we want, the bubble sort makes $\\frac{N^{2}}{2}$\n",
    "comparisons and swaps. So has a complexity equal to $O$($N^{2}$)\n",
    "\n",
    "__MergeSort__ : Given an array of size N,N>1, the merge sort algorithm makes several steps to sort the algorithm. It's a recursive algorithm:\n",
    "At the first step (i=1) merges array of size 1, at the second step (i=2) merges array of size 2, at the i-th step merges array of size $2^{i-1}$.\n",
    "To merge two arrays of same size N takes only one loop over the two array so it take $O$($N$).\n",
    "The last step occurs when merges array of size  $\\frac{N}{2}$, so the last step occurs when $2^{i-1}$ = $\\frac{N}{2}$   ==> i-1 = $\\log{_2}{\\frac{N}{2}}$ ==> i is $O$($\\log{N}$)\n",
    "In conclusion the algorithm has to merge two arrays $\\log{N}$ times, so has time complexity $O$(N $\\log{N}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sorting import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert how many rows you want to sort, insert 0 to read all the text rows.\n",
      "Time to read N = 50000 rows: 10.293111085891724 seconds\n"
     ]
    }
   ],
   "source": [
    "print('Insert how many rows you want to sort, insert 0 to read all the text rows.')\n",
    "N = int(input())\n",
    "start = time.time()\n",
    "rows,N = readTxt(N)\n",
    "end  =  time.time()\t\n",
    "print(\"Time to read N = \"+str(N)+\" rows: \"+str(end-start)+\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection takes: 496.15750432014465 seconds to sort N = 50000 rows!\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "unsorted_sel=  rows\n",
    "sel_sort(unsorted_sel,N)\n",
    "end  =  time.time()\t\n",
    "sel_time = end-start\n",
    "print(\"Selection takes: \"+str(sel_time)+\" seconds to sort N = \"+str(N)+\" rows!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bubble Sort takes: 1293.1193449497223 seconds to sort N = 50000 rows!\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "unsorted_bubble=  rows\n",
    "bubble_sort(unsorted_bubble,N)\n",
    "end  =  time.time()\t\n",
    "bub_time = end-start\n",
    "print(\"Bubble Sort takes: \"+str(bub_time)+\" seconds to sort N = \"+str(N)+\" rows!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge Sort takes: 0.5184693336486816 seconds to sort N = 50000 rows!\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "unsorted_merge = rows\n",
    "merge_sort(unsorted_merge)\n",
    "end  =  time.time()\t\n",
    "merge_time = end-start\n",
    "print(\"Merge Sort takes: \"+str(merge_time)+\" seconds to sort N = \"+str(N)+\" rows!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_Txt(sorted_values):\n",
    "    with open('RankingLists.txt','w') as output:\n",
    "        for elem in sorted_values:\n",
    "            output.write(elem[0]+' '+elem[1]+' '+str(elem[2])+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_Txt(unsorted_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most time efficient algorithm between the 3 we implemented is the __mergeSort__ algorithm. This algorithm has a time complexity much smaller than the complexity of __bubbleSort__ and __selectionSort__. This statement is verified just by seeing that to complete the query on the ApplicantsInfo.txt the __mergeSort__ completes the task in less than a second ( 0.52 seconds), while the __selectionSort__ needs about  8 minutes and 16.16 seconds (496.16 seconds) and the  __bubbleSort__ needs 21 minutes and 33.12 seconds (1293.12 seconds)!\n",
    "\n",
    "The worst time efficient algorithm is the __bubbleSort__, even though it has the same time complexity as the __selectionSort__.\n",
    "However the __bubbleSort__ needs almost the double of the time needed by the __selectionSort__ to complete the task. This occurs because the __bubbleSort__ makes more swap operation, roughly the double, of entries than the __selectionSort__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MapReduce Implementations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapReduce(values):\n",
    "    counter  = Counter()\n",
    "    for elem in values:\n",
    "        v= elem[2]\n",
    "        k = elem[0]+' '+elem[1]\n",
    "        counter[k] = v\n",
    "    \n",
    "    return counter\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapReduce takes 0.04696989059448242 seconds to sort 50000 rows\n"
     ]
    }
   ],
   "source": [
    "start  =time.time()\n",
    "unsorted_map = rows\n",
    "mapReduce(unsorted_map)\n",
    "end = time.time()\n",
    "print(\"MapReduce takes \"+str(end-start)+\" seconds to sort \"+str(N)+\" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My __MapReduce__ algorithm implementation, which uses the counter data structure from the collections module, is even faster than the __mergeSort__ algorithm. \n",
    "\n",
    "__MapReduce__ needs only 0.047 seconds to sort 50000 rows, while __mergeSort__ algorithm needs 0.52 seconds, so __mapReduce__ is one order of magnitude faster than the __mergeSort__ algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fe61594923d2a5d982534718f3a44d80d54aa84bc222edb79bd8e8f2cce8cae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
