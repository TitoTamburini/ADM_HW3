{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Places of the world\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection\n",
    "\n",
    "For this homework, there is no provided dataset. Instead, you have to build your own. Your search engine will run on text documents. So, here\n",
    "we detail the procedure to follow for the data collection. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of places\n",
    "\n",
    "We start with the list of places to include in your corpus of documents. In particular, we focus on the [Most popular places](https://www.atlasobscura.com/places?sort=likes_count). Next, we want you to **collect the URL** associated with each site in the list from this list.\n",
    "The list is long and split into many pages. Therefore, we ask you to retrieve only the URLs of the places listed in **the first 400 pages** (each page has 18 places, so that you will end up with 7200 unique place URLs).\n",
    "\n",
    "The output of this step is a `.txt` file whose single line corresponds to the place's URL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built the urls i need to get every url for every places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url  = 'https://www.atlasobscura.com/places?sort=likes_count'\n",
    "main_domain  ='https://www.atlasobscura.com'\n",
    "get_page_query = '/places?page='\n",
    "query_end = '&sort=likes_count'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code to get every url of evry places and the save them locally in places_url.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_url  = []\n",
    "output = open(\"places_url.txt\",'w')\n",
    "cnt  =  0\n",
    "for i in range(1,401):\n",
    "    try:\n",
    "        req  = requests.get(main_domain+get_page_query+str(i)+query_end)\n",
    "        soup  = BeautifulSoup(req.text)\n",
    "        places_url= [main_domain+ x.get('href') for x in soup.find_all('a',{'class' : 'content-card-place' })]\n",
    "        for url in places_url:\n",
    "            output.write(url+\"\\n\")\n",
    "            cnt+=1\n",
    "    except Exception as e:\n",
    "        print(\"Error Occured: \"+e)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all the 7200 urls have been written in places_url.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200\n"
     ]
    }
   ],
   "source": [
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl places\n",
    "\n",
    "Once you get all the URLs in the first 400 pages of the list, you:\n",
    "\n",
    "1. Download the HTML corresponding to each of the collected URLs.\n",
    "2. After you collect a single page, immediately save its `HTML` in a file. In this way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
    "3. Organize the entire set of downloaded `HTML` pages into folders. Each folder will contain the `HTML` of the places on page 1, page 2, ... of the list of locations.\n",
    "\n",
    "__Tip__: Due to a large number of pages you should download, you can use some methods that can help you shorten the time it takes. If you employed a particular process or approach, kindly describe it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_html function\n",
    "\n",
    "In this function there is the code to download the page html of a place on www.atlasobscura.com:\n",
    "\n",
    "*Parameters*\n",
    "\n",
    "    __url__ : this url is the location of the html page that we want to download, it is read from a file txt containing 7200 url of places on www.atlasobscura.com.\n",
    "    \n",
    "    __path__: this path is the location where we want to save the .html file locally.\n",
    "\n",
    "    __number__: this number goes from 0 to 7199, each number corresponds to a different place.\n",
    "    \n",
    "    __page__: this number goes from 1 to 400, each number corresponds to a page from this link https://www.atlasobscura.com/places?page=<page>sort=likes_count.\n",
    "\n",
    "*Execution*\n",
    "    \n",
    "    This function makes a GET request to the given <url>, tries to create a new html file called place<number>.html in the <path> given as input.\n",
    "    When the GET request has a status code equals to 200, the request.text attribute is written in the place<number>.html. \n",
    "    After the file is written, it gives a feedback if the file as already been saved locally( in this case you will see on the standard output \"Done!\") or if it has just been saved locally(in this case you will see on the standard output \"Downloaded place <number>, Page <page>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url,path,number,page):\n",
    "    try:    \n",
    "        if not os.path.exists(path+\"/place\"+str(number)+\".html\") or os.path.getsize(path+\"/place\"+str(number)+\".html\") < 2000:\n",
    "            with open(path+\"/place\"+str(number)+\".html\",\"w\", encoding=\"utf-8\") as file_html:\n",
    "                req =  requests.get(url)\n",
    "                if req.status_code == 200:\n",
    "                    file_html.write(req.text)\n",
    "                    print(\"Downloaded place \"+str(number)+\", Page \"+str(page))\n",
    "        else:\n",
    "            print(\"done!\")\n",
    "    except Exception as e:\n",
    "        print(\"Error occured! \"+ str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code creates the need 400 directory correponding to each page from this link  https://www.atlasobscura.com/places?page=<page_index>sort=likes_count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_index  in range(1,401):\n",
    "    dir_path  = \"HTML_FILES/page\"+str(page_index)\n",
    "    if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code the calls get_html for every url saved in the places_url.txt.\n",
    "I used linecache module to acces directly to a specific line in the file.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,7200):\n",
    "    page  = (i//18)+1\n",
    "    dir_path  = \"HTML_FILES/page\"+str(page)\n",
    "    url = str(linecache.getline('places_url.txt',i+1).replace('\\n',\"\"))\n",
    "    get_html(url,dir_path,i,page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n",
    "\n",
    "At this point, you should have all the HTML documents about the places of interest, and you can start to extract the places' information. The list of the information we desire for each place and their format is as follows:\n",
    "\n",
    "1. Place Name (to save as `placeName`): String.\n",
    "2. Place Tags (to save as `placeTags`): List of Strings.\n",
    "3. \\# of people who have been there (to save as `numPeopleVisited`): Integer.\n",
    "4. \\# of people who want to visit the place(to save as `numPeopleWant`): Integer.\n",
    "5. Description (to save as `placeDesc`): String. Everything from under the first image up to \"know before you go\" (orange frame on the example image).\n",
    "6. Short Description (to save as `placeShortDesc`): String. Everything from the title and location up to the image (blue frame on the example image).\n",
    "7. Nearby Places (to save as `placeNearby`): Extract the names of all nearby places, but only keep unique values: List of Strings.\n",
    "8. Address of the place(to save as `placeAddress`): String.\n",
    "9. Altitude and Longitude of the place's location(to save as `placeAlt` and `placeLong`): Integers\n",
    "10. The username of the post editors (to save as `placeEditors`): List of Strings.\n",
    "11. Post publishing date (to save as `placePubDate`): datetime.\n",
    "12. The names of the lists that the place was included in (to save as `placeRelatedLists`): List of Strings.\n",
    "13. The names of the related places (to save as `placeRelatedPlaces`): List of Strings.\n",
    "14. The URL of the page of the place (to save as `placeURL`):String\n",
    "<p align=\"center\">\n",
    "<img src=\"img/last_version_place.png\" width = 1000>\n",
    "</p>\n",
    "\n",
    "\n",
    "For each place, you create a `place_i.tsv` file of this structure:\n",
    "\n",
    "```\n",
    "placeName \\t placeTags \\t  ... \\t placeURL\n",
    "```\n",
    "\n",
    "If an information is missing, you just leave it as an empty string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from datetime import datetime\n",
    "import linecache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_index  in range(1,401):\n",
    "    dir_path  = \"TSV_FILES/page\"+str(page_index)\n",
    "    if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_placeEditors(soup):\n",
    "    editors_list = []\n",
    "    soup_obj  = soup.findAll('li',{'class':'DDPContributorsList__item'})\n",
    "    for li in soup_obj:\n",
    "        editors_list.append(li.find('span').text)\n",
    "    return editors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tsv(place_number,tsv_path,html_path,url):\n",
    "    if not os.path.exists(tsv_path+\"/place_\"+str(place_number)+'.csv'): \n",
    "        try:\n",
    "            with open(html_path,\"r\",encoding='utf-8') as html:\n",
    "                soup = BeautifulSoup(html)\n",
    "                row = \"\"\n",
    "                placeName = soup.find('h1',{'class' : 'DDPage__header-title' }).text.strip()\n",
    "                row += placeName+' \\t'\n",
    "                placeTags  = [x.text.replace('\\n',\"\") for x in soup.find_all('a',{'class' : 'itemTags__link' })]\n",
    "                row += str(placeTags)+' \\t'\n",
    "                people = list(dict.fromkeys([int(x.text.strip()) for x in soup.find_all('div',{'class':'title-md item-action-count'})]))\n",
    "                row += ' '+str(people[0])+' \\t'#people[0]  = numPeopleVisited\n",
    "                row +=' '+ str(people[1])+' \\t'#people[1]  = numPeopleWant\n",
    "                placeDesc = [x.text.replace(\"\\xa0\",\" \") for x in soup.find('div',{'id': 'place-body'}).findChildren('p')]\n",
    "                row += ' '+''.join(placeDesc)+' \\t'\n",
    "                placeShortDesc = soup.find('h3',{'class' :'DDPage__header-dek'}).text.strip()\n",
    "                row += ' '+placeShortDesc+' \\t'\n",
    "                placeNearby = [x.text for x in soup.find_all('div',{'class':'DDPageSiderailRecirc__item-title'})]\n",
    "                row+=' '+str(placeNearby)+' \\t'\n",
    "                address_coordinates = [x for x in soup.find('address',{'class':'DDPageSiderail__address'}).find('div',{'class':None},recursive=False).text.replace('\\n',\"\\t\").split('\\t') if x != '']\n",
    "                placeAddress  = address_coordinates[0]\n",
    "                row +=' '+placeAddress+' \\t'\n",
    "                coordinates= address_coordinates[1].replace(',','').split()\n",
    "                placeAlt = int(float(coordinates[0]))\n",
    "                placeLong = int(float(coordinates[1]))\n",
    "                row +=' '+str(placeAlt)+' \\t'\n",
    "                row +=' '+str(placeLong)+' \\t'\n",
    "                placeEditors  =get_placeEditors(soup)\n",
    "                row +=' '+str(placeEditors)+' \\t'\n",
    "                placePubDate = datetime.strptime(soup.find('div',{'class':'DDPContributor__name'}).text,'%B %d, %Y').date()\n",
    "                row +=' '+str(placePubDate)+' \\t'\n",
    "                div = soup.findAll('div',{'class':'card-grid CardRecircSection__card-grid js-inject-gtm-data-in-child-links'})\n",
    "                all_A = [x.findAll('a',{'class':'Card --content-card-v2 --content-card-item'}) for x in div][1]\n",
    "                placeRelatedPlaces = [x.find('span').text for x in all_A]\n",
    "                row += ' '+str(placeRelatedPlaces)+' \\t'\n",
    "                if(len(div) > 2):\n",
    "                    placeRelatedLists = [x.find('span').text for x in div[2].findAll('a')]\n",
    "                else:\n",
    "                    placeRelatedLists =\"\"\n",
    "                row += ' '+str(placeRelatedLists)+' \\t'\n",
    "                row += ' '+url+' \\t'\n",
    "                try:\n",
    "                        with open(tsv_path+\"/place_\"+str(place_number)+'.csv','w',encoding='utf-8') as f:\n",
    "                            f.write(row)\n",
    "                            print(\"csv created!\")\n",
    "                except  Exception as e:\n",
    "                    print(\"File not Created: \"+e)\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred: \"+str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place 817,page 46\n",
      "Error occurred: list index out of range\n",
      "Place 1212,page 68\n",
      "Error occurred: list index out of range\n",
      "Place 1666,page 93\n",
      "Error occurred: list index out of range\n",
      "Place 1702,page 95\n",
      "Error occurred: list index out of range\n",
      "Place 1842,page 103\n",
      "Error occurred: could not convert string to float: 'Brooklyn'\n",
      "Place 2728,page 152\n",
      "Error occurred: list index out of range\n",
      "Place 2871,page 160\n",
      "Error occurred: list index out of range\n",
      "Place 3325,page 185\n",
      "Error occurred: list index out of range\n",
      "Place 3410,page 190\n",
      "Error occurred: list index out of range\n",
      "Place 3416,page 190\n",
      "Error occurred: list index out of range\n",
      "Place 3498,page 195\n",
      "Error occurred: list index out of range\n",
      "Place 3541,page 197\n",
      "Error occurred: list index out of range\n",
      "Place 3638,page 203\n",
      "Error occurred: list index out of range\n",
      "Place 3689,page 205\n",
      "Error occurred: list index out of range\n",
      "Place 3839,page 214\n",
      "Error occurred: list index out of range\n",
      "Place 3939,page 219\n",
      "Error occurred: list index out of range\n",
      "Place 3956,page 220\n",
      "Error occurred: list index out of range\n",
      "Place 4097,page 228\n",
      "Error occurred: list index out of range\n",
      "Place 4172,page 232\n",
      "Error occurred: list index out of range\n",
      "Place 4364,page 243\n",
      "Error occurred: list index out of range\n",
      "Place 4433,page 247\n",
      "Error occurred: list index out of range\n",
      "Place 4645,page 259\n",
      "Error occurred: list index out of range\n",
      "Place 4724,page 263\n",
      "Error occurred: list index out of range\n",
      "Place 5063,page 282\n",
      "Error occurred: list index out of range\n",
      "Place 5207,page 290\n",
      "Error occurred: list index out of range\n",
      "Place 5288,page 294\n",
      "Error occurred: list index out of range\n",
      "Place 5308,page 295\n",
      "Error occurred: list index out of range\n",
      "Place 5521,page 307\n",
      "Error occurred: list index out of range\n",
      "Place 5572,page 310\n",
      "Error occurred: list index out of range\n",
      "Place 5783,page 322\n",
      "Error occurred: list index out of range\n",
      "Place 5877,page 327\n",
      "Error occurred: list index out of range\n",
      "Place 5953,page 331\n",
      "Error occurred: could not convert string to float: 'rue'\n",
      "Place 6044,page 336\n",
      "Error occurred: 'NoneType' object has no attribute 'text'\n",
      "Place 6088,page 339\n",
      "Error occurred: list index out of range\n",
      "Place 6120,page 340\n",
      "Error occurred: list index out of range\n",
      "Place 6268,page 349\n",
      "Error occurred: list index out of range\n",
      "Place 6366,page 354\n",
      "Error occurred: list index out of range\n",
      "Place 6430,page 358\n",
      "Error occurred: list index out of range\n",
      "Place 6482,page 361\n",
      "Error occurred: list index out of range\n",
      "Place 6521,page 363\n",
      "Error occurred: list index out of range\n",
      "Place 6879,page 383\n",
      "Error occurred: list index out of range\n",
      "Place 7029,page 391\n",
      "Error occurred: list index out of range\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,7200):\n",
    "    page  = (i//18)+1\n",
    "    html_path = \"HTML_FILES/page\"+str(page)+\"\\place\"+str(i)+\".html\"\n",
    "    tsv_path  = \"TSV_FILES/page\"+str(page)\n",
    "    url  = str(linecache.getline('places_url.txt',i+1)).replace('\\n',\"\")\n",
    "    make_tsv(i+1,tsv_path,html_path,url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fe61594923d2a5d982534718f3a44d80d54aa84bc222edb79bd8e8f2cce8cae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
